Hereâ€™s an improved version of your GitHub README in Markdown, incorporating better structure, formatting, and clarity.

â¸»

ğŸš€ CIS5300 Final Project: From BART to Edge

Fine-tuning Large Language Models (LLMs) for Chinese-to-English translation in the education domain, with a comparison of mBART and M2M100, utilizing LoRA and Layer Freezing techniques to optimize performance and efficiency.

â¸»

ğŸ“Œ Project Overview

ğŸ”¹ Objective: Identify the most effective and resource-efficient fine-tuning technique for domain-specific Chinese-to-English translation.
ğŸ”¹ Models Compared:
	â€¢	mBART-50 (Multilingual BART)
	â€¢	M2M100 (Many-to-Many Multilingual Model)
ğŸ”¹ Techniques Implemented:
	â€¢	LoRA (Low-Rank Adaptation)
	â€¢	Layer Freezing
	â€¢	Quantization (LLaMA & Marian MT)

ğŸ“ Project Structure

The repository contains two main Jupyter notebooks:
	â€¢	Education_Translation.ipynb â†’ Fine-tuning in the education domain
	â€¢	Science_Translation.ipynb â†’ Fine-tuning in the science domain

Pipeline Overview

1ï¸âƒ£ Load & Preprocess: Import libraries, load datasets, initialize models, define evaluation metrics.
2ï¸âƒ£ Baseline Evaluation: Evaluate pre-trained models (mBART & M2M100) on a 50,000-sample dataset.
3ï¸âƒ£ Hyperparameter Tuning: Grid search on a smaller 10,000-sample dataset to find optimal settings.
4ï¸âƒ£ Fine-Tuning: Apply best parameters for LoRA and Layer Freezing on the full dataset.
5ï¸âƒ£ Evaluation & Analysis: Compare BLEU scores, perform error analysis, and measure GPU usage.
6ï¸âƒ£ Extensions: Explore quantization using LLaMA and Marian MT for low-resource scenarios.

â¸»

ğŸ”§ Setup & Installation

1ï¸âƒ£ Install Dependencies

Ensure the following libraries are installed:

pip install datasets optimum auto-gptq sentencepiece bitsandbytes sacremoses sacrebleu transformers peft nltk tqdm pandas torch

2ï¸âƒ£ Download Dataset

Place the dataset in the working directory and update file paths accordingly.

Dataset File	Domain
Bi-Education.txt	Education
Bi-Science.txt	Science



â¸»

ğŸ“Š Results & Findings

Baseline BLEU Scores (Before Fine-Tuning)

Model	Science	Education
mBART	0.1223	0.1169
M2M100	0.0231	0.0347

Fine-Tuned BLEU Scores

Model	Science	Education	Improvement (%)
mBART	0.1997	0.1192	+63.28% (Sci)
M2M100	0.1200	0.0809	+419.48% (Sci)

LoRA vs Layer Freezing Performance

Model	Domain	Fine-Tuned BLEU	LoRA BLEU	Layer Freezing BLEU
mBART	Science	0.1997	0.1352	0.1597
mBART	Education	0.1192	0.1153	0.1178
M2M100	Science	0.1200	0.0976	0.1118
M2M100	Education	0.0809	0.0782	N/A

Key Insights:
âœ… Fine-tuning achieved the highest BLEU scores.
âœ… Layer Freezing performed better than LoRA across models.
âœ… LoRA is more efficient, but sacrifices some translation quality.

â¸»

âš¡ GPU Usage Analysis

Memory consumption on NVIDIA A100 (40GB VRAM)

Method	mBART (GB)	M2M100 (GB)
Fine-Tuning	7.0	5.5
LoRA	2.0	2.0
Layer Freezing	4.5	3.5

Key Observations:
âœ” LoRA is the most memory-efficient, making it ideal for low-resource scenarios.
âœ” Layer Freezing reduces memory usage while maintaining decent BLEU scores.
âœ” Fine-Tuning achieves the best results but is computationally expensive.

â¸»

â— Error Analysis

We categorized errors into Word-Level, Structural, and Other Errors.

Common Issues Identified

Model	Word-Level Errors	Structural Errors
M2M100 (LoRA)	3,960	2,057
M2M100 (Fine-Tuned)	3,884	2,085
mBART (LoRA)	~3,250	~1,300
mBART (Fine-Tuned)	~3,250	~1,300



â¸»

ğŸ† Conclusion & Recommendations

âœ… Best Overall Performance: Fine-Tuned mBART (Highest BLEU Score)
âš¡ Best Resource-Efficient Model: LoRA M2M100 (Low GPU Usage, Competitive BLEU)
ğŸ”¬ Best Balance of Efficiency & Quality: Layer Freezing

Final Takeaways:
	â€¢	For high-quality translation: Use fully fine-tuned models.
	â€¢	For low-resource deployment: LoRA is highly efficient, but may sacrifice some translation accuracy.
	â€¢	For an optimal trade-off: Layer Freezing provides a good balance between performance and efficiency.

Future Work
	â€¢	Hybrid techniques: Combining LoRA with Layer Freezing.
	â€¢	Improved quantization methods: Exploring 8-bit and 4-bit models.
	â€¢	Dataset expansion: Using out-of-domain data to enhance robustness.

â¸»

ğŸ“ Contributors & Acknowledgments

This project was developed as part of CIS5300 at University of Pennsylvania.
ğŸ“– Authors: Xuanyou Liu, Prekshi Vyas, Raksha Ramesh, Manurag Khullar.

Special thanks to:
ğŸ‘¨â€ğŸ« Professor Mark Yatskar (Instructor)
ğŸ§‘â€ğŸ« Ugurcan Vurgun (TA & Mentor)

â¸»

ğŸ“œ References

ğŸ“Œ Main Papers & Resources:
	â€¢	Hu et al. â€œLoRA: Low-Rank Adaptation of Large Language Modelsâ€ (ICLR 2022)
	â€¢	Papineni et al. â€œBLEU: Automatic Evaluation of Machine Translationâ€ (ACL 2002)
	â€¢	Tian et al. â€œUM-Corpus: A Large English-Chinese Parallel Corpusâ€ (LREC 2014)

â¸»

ğŸ’¡ If you found this helpful, give it a â­ on GitHub! ğŸš€
Let us know your thoughts and suggestions in the Issues section.

â¸»

This improved Markdown enhances readability, introduces tables, sections, and highlights key findings effectively. Let me know if you need further tweaks! ğŸš€
